# Articulatory Feature Prediction from Surface EMG during Speech Production
### Jihwan Lee<sup>1</sup>, Kevin Huang<sup>1</sup>, Kleanthis Avramidis<sup>1</sup>, Simon Pistrosch<sup>2</sup>, Monica Gonzalez-Machorro<sup>2</sup>, Yoonjeong Lee<sup>1</sup>, Björn Schuller<sup>2</sup>, Louis Goldstein<sup>3</sup>, Shrikanth Narayanan<sup>1</sup>

#### <sup>1</sup>Signal Analysis and Interpretation Laboratory, University of Southern California, USA <br> <sup>2</sup>CHI – Chair of Health Informatics, Technical University of Munich, Germany<br> <sup>3</sup>Department of Linguistics, University of Southern California, USA

### Code implementation of [paper](https://arxiv.org/abs/2505.13814 "paper link") (Interspeech 2025)

### Abstract
We present a model for predicting articulatory features from surface electromyography (EMG) signals during speech production. The proposed model integrates convolutional layers and a Transformer block, followed by separate predictors for articulatory features. Our approach achieves a high prediction correlation of approximately 0.9 for most articulatory features. Furthermore, we demonstrate that these predicted articulatory features can be decoded into intelligible speech waveforms. To our knowledge, this is the first method to decode speech waveforms from surface EMG via articulatory features, offering a novel approach to EMG-based speech synthesis. Additionally, we analyze the relationship between EMG electrode placement and articulatory feature predictability, providing knowledge-driven insights for optimizing EMG electrode configurations. The source code and decoded speech samples are publicly available.

<!-- ![overall_architecture](figures/emg_archi.png) -->

## Sample Page

Speech samples are available [here](https://lee-jhwn.github.io/IS25-emg-ema/ "sample page").

## Data



## Preprocessing




## References

